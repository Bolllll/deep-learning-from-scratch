{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04/gradient_1d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    #y = f(x) - d*x\n",
    "    #return lambda t: d*t + y\n",
    "    return lambda t: d*t\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04/gradient_2d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)  # f(x+h)\n",
    "        \n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)  # f(x-h)\n",
    "\n",
    "        # grad(xi)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val  # 値を元に戻す # 恢复到初始的值\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1) # 实际可以是 y = x1^2+x2^2+x3^2....\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "\n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]).T).T\n",
    "\n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04/gradient_method.py\n",
    "- 如下代码的作用是展示如何求函数的最小值 - \n",
    "- 比如 f = (x1+1)^2 + (x2-1)^2 ,可以看到函数最终会到接近0的位置，并且找到x1和x2，其实对于其他函数来说也是可以的。\n",
    "- 但有些函数在这个区域是单调的，\n",
    "- 比如椭圆1=x^2/a+y^2/b - 1，以及 y = x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    # return x[0]**2 + x[1]**2\n",
    "    return x[0]**2/2 + x[1]**2/4 - 1 # f = x1^2/a + x2^2/b - 1\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "lr = 0.1\n",
    "step_num = 50\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()\n",
    "print(x_history)\n",
    "for h in x_history:\n",
    "    print(function_2(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04/gradient_simplenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04/two_layer_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04/train_neuralnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.8205666666666667, 0.8222\n",
      "train acc, test acc | 0.8838833333333334, 0.8874\n",
      "train acc, test acc | 0.8985166666666666, 0.9031\n",
      "train acc, test acc | 0.9078833333333334, 0.9102\n",
      "train acc, test acc | 0.9111, 0.9154\n",
      "train acc, test acc | 0.91645, 0.9178\n",
      "train acc, test acc | 0.9205833333333333, 0.9245\n",
      "train acc, test acc | 0.9250333333333334, 0.9268\n",
      "train acc, test acc | 0.9276, 0.9292\n",
      "train acc, test acc | 0.9305333333333333, 0.9323\n",
      "train acc, test acc | 0.9335833333333333, 0.9349\n",
      "train acc, test acc | 0.93635, 0.937\n",
      "train acc, test acc | 0.9390166666666667, 0.9376\n",
      "train acc, test acc | 0.94065, 0.9403\n",
      "train acc, test acc | 0.9436666666666667, 0.9417\n",
      "train acc, test acc | 0.9454, 0.9436\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 随机取数据 - 但是有些数据可能不会被用到\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    # 顺序取数据\n",
    "    # k = i % train_size/batch_size\n",
    "    # x_batch = x_train[k*batch_size:(k+1)*batch_size]\n",
    "    # t_batch = t_train[k*batch_size:(k+1)*batch_size]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
